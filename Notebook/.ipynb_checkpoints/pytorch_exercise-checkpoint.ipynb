{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Play with Pytorch\n",
    "## 变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n",
      "tensor([[1., 2.],\n",
      "        [3., 4.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.FloatTensor([[1,2], [3,4]])\n",
    "var = Variable(tensor, requires_grad=True)\n",
    "print(tensor)\n",
    "print(var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 计算梯度　　　　　　　"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7.5000)\n",
      "tensor(7.5000, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "t_out = torch.mean(tensor * tensor) # * means element-wise product in torch\n",
    "v_out = torch.mean(var * var)\n",
    "\n",
    "print(t_out)\n",
    "print(v_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5000, 1.0000],\n",
      "        [1.5000, 2.0000]])\n",
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n",
      "[[1. 2.]\n",
      " [3. 4.]]\n"
     ]
    }
   ],
   "source": [
    "v_out.backward(retain_graph=True)\n",
    "print(var.grad)\n",
    "print(var.data)\n",
    "print(var.data.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 激活函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/huiwen/anaconda3/envs/baselines/lib/python3.6/site-packages/torch/nn/functional.py:1386: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/huiwen/anaconda3/envs/baselines/lib/python3.6/site-packages/torch/nn/functional.py:1374: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "/home/huiwen/anaconda3/envs/baselines/lib/python3.6/site-packages/matplotlib/cbook/deprecation.py:106: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
      "  warnings.warn(message, mplDeprecation, stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "x = torch.linspace(-5, 5., 200)\n",
    "x = Variable(x)\n",
    "\n",
    "x_np = x.data.numpy()\n",
    "\n",
    "y_relu = F.relu(x).data.numpy()\n",
    "y_sig = F.sigmoid(x).data.numpy()\n",
    "y_tanh = F.tanh(x).data.numpy()\n",
    "y_softplus = F.softplus(x).data.numpy()\n",
    "\n",
    "# plot\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(1, figsize=(8,6))\n",
    "plt.subplot(221)\n",
    "plt.plot(x_np, y_relu, c='red', label='relu')\n",
    "plt.ylim((-1, 5))\n",
    "plt.legend(loc='best')\n",
    "\n",
    "plt.subplot(222)\n",
    "plt.plot(x_np, y_sig, c='red', label='sigmoid')\n",
    "plt.ylim((-0.2, 1.2))\n",
    "plt.legend(loc='best')\n",
    "\n",
    "plt.subplot(223)\n",
    "plt.plot(x_np, y_tanh, c='red', label='tanh')\n",
    "plt.ylim((-1.2, 1.2))\n",
    "plt.legend(loc='best')\n",
    "\n",
    "plt.subplot(224)\n",
    "plt.plot(x_np, y_softplus, c='red', label='softplus')\n",
    "plt.ylim((-0.2, 6))\n",
    "plt.legend(loc='best')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# samples\n",
    "x = torch.unsqueeze(torch.linspace(-1,1,100),dim=1)\n",
    "y = x.pow(2) + 0.2*torch.rand(x.size())\n",
    "\n",
    "plt.scatter(x.data.numpy(), y.data.numpy())\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (hidden): Linear(in_features=1, out_features=10, bias=True)\n",
      "  (pred): Linear(in_features=10, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, n_feat, n_hidden, n_out):\n",
    "        super(Net, self).__init__()\n",
    "        self.hidden = torch.nn.Linear(n_feat, n_hidden)\n",
    "        self.pred = torch.nn.Linear(n_hidden, n_out)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.hidden(x))\n",
    "        x = self.pred(x)\n",
    "        return x\n",
    "    \n",
    "net = Net(n_feat=1, n_hidden=10, n_out=1)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "show() got an unexpected keyword argument 'block'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-92-3d762b87935e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r-'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Loss=%.4f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfontdict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'color'\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;34m'red'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpause\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/baselines/lib/python3.6/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mpause\u001b[0;34m(interval)\u001b[0m\n\u001b[1;32m    289\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstale\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m             \u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw_idle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m         \u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m         \u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_event_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minterval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/baselines/lib/python3.6/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \"\"\"\n\u001b[1;32m    250\u001b[0m     \u001b[0;32mglobal\u001b[0m \u001b[0m_show\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_show\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: show() got an unexpected keyword argument 'block'"
     ]
    }
   ],
   "source": [
    "# train the net\n",
    "optimizer =torch.optim.SGD(net.parameters(), lr=0.2)\n",
    "loss_func = torch.nn.MSELoss()\n",
    "plt.ion()   # 画图\n",
    "plt.show()\n",
    "\n",
    "for t in range(100):\n",
    "    pred = net(x)\n",
    "    loss = loss_func(pred, y)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if t % 5 == 0:\n",
    "        # plot and show learning process\n",
    "        plt.cla()\n",
    "        plt.scatter(x.data.numpy(), y.data.numpy())\n",
    "        plt.plot(x.data.numpy(), pred.data.numpy(), 'r-', lw=5)\n",
    "        plt.text(0.5, 0, 'Loss=%.4f' % loss.data.numpy(), fontdict={'size': 20, 'color':  'red'})\n",
    "        plt.pause(0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ｄata = torch.ones(100, 2)\n",
    "x0 = torch.normal(2*data,1) # mean is 2, std is 1\n",
    "x0.data.numpy()\n",
    "y0 = torch.zeros(100)\n",
    "\n",
    "x1 = torch.normal(-2*data, 1)\n",
    "y1 = torch.ones(100)\n",
    "\n",
    "x = torch.cat((x0, x1), 0).type(torch.FloatTensor)\n",
    "x.size()\n",
    "y = torch.cat((y0, y1), ).type(torch.LongTensor)\n",
    "y.size()\n",
    "# # 画图\n",
    "plt.scatter(x.data.numpy()[:,0], x.data.numpy()[:, 1] , c=y.data.numpy(), s=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the net\n",
    "class ClaNet(torch.nn.Module):\n",
    "    def __init__(self, n_feat, n_hidden, n_out):\n",
    "        super(ClaNet, self).__init__()\n",
    "        self.hidden = torch.nn.Linear(n_feat, n_hidden)\n",
    "        self.out = torch.nn.Linear(n_hidden, n_out)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.hidden(x))\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "    \n",
    "net = ClaNet(n_feat=2, n_hidden=10, n_out=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "%matplotlib inline\n",
    "# from IPython import display\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.02)\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "plt.ion()   # 画图\n",
    "# plt.show()\n",
    "# fig = plt.figure()\n",
    "# ax = fig.add_subplot(1,1,1)\n",
    "# ax.scatter(x.data.numpy()[:,0], x.data.numpy()[:, 1] , c=y.data.numpy(), s=50)\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "for t in range(100):\n",
    "    out = net(x)     # 喂给 net 训练数据 x, 输出分析值\n",
    "\n",
    "    loss = loss_func(out, y)     # 计算两者的误差\n",
    "\n",
    "    optimizer.zero_grad()   # 清空上一步的残余更新参数值\n",
    "    loss.backward()         # 误差反向传播, 计算参数更新值\n",
    "    optimizer.step()        # 将参数更新值施加到 net 的 parameters 上\n",
    "    if t % 2 == 0:\n",
    "        plt.cla()\n",
    "        # 过了一道 softmax 的激励函数后的最大概率才是预测值\n",
    "        prediction = torch.max(F.softmax(out), 1)[1]\n",
    "        pred_y = prediction.data.numpy().squeeze()\n",
    "        target_y = y.data.numpy()\n",
    "        plt.scatter(x.data.numpy()[:, 0], x.data.numpy()[:, 1], c=pred_y, s=100, lw=0, cmap='RdYlGn')\n",
    "        accuracy = sum(pred_y == target_y)/200.  # 预测中有多少和真实值一样\n",
    "        plt.text(1.5, -4, 'Accuracy=%.2f' % accuracy, fontdict={'size': 20, 'color':  'red'})\n",
    "        plt.pause(0.1)\n",
    "\n",
    "plt.ioff()  # 停止画图\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:baselines]",
   "language": "python",
   "name": "conda-env-baselines-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
